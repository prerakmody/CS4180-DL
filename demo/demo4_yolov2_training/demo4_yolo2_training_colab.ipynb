{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Start - Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Step1 - Get repo\n",
    "! rm -rf CS4180-DL\n",
    "! git clone --branch feature/prerak2 https://github.com/prerakmody/CS4180-DL\n",
    "\n",
    "# Step2 - Get Dataset\n",
    "! wget -P /content/CS4180-DL/data/dataset https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\n",
    "! tar xf /content/CS4180-DL/data/dataset/VOCtrainval_11-May-2012.tar --directory /content/CS4180-DL/data/dataset\n",
    "! wget -P /content/CS4180-DL/data/dataset https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
    "! tar xf /content/CS4180-DL/data/dataset/VOCtrainval_06-Nov-2007.tar --directory /content/CS4180-DL/data/dataset\n",
    "! wget -P /content/CS4180-DL/data/dataset https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
    "! tar xf /content/CS4180-DL/data/dataset/VOCtest_06-Nov-2007.tar --directory /content/CS4180-DL/data/dataset\n",
    "! mkdir /content/CS4180-DL/data/weights\n",
    "\n",
    "! wget -P /content/CS4180-DL/data/weights https://pjreddie.com/media/files/yolov2-voc.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T09:28:23.399092Z",
     "start_time": "2019-05-28T09:28:18.026766Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      " - year :  2012  || image_set :  train\n",
      " - year :  2012  || image_set :  val\n",
      " - year :  2007  || image_set :  train\n",
      " - year :  2007  || image_set :  val\n",
      " - year :  2007  || image_set :  test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## Step3 - Add path to main dir\n",
    "DIR_MAIN = os.path.abspath('CS4180-DL/') #'../../../CS4180-DL/'\n",
    "sys.path.append(DIR_MAIN)\n",
    "\n",
    "## Step4 - Download pruned weights from GDrive\n",
    "from src.utils import download_gdrive\n",
    "FILE_ID =  '1dHnUQ8G3GObZSMh9eQ0zUR5wor0ttW3U'\n",
    "DIR_WEIGHTS = os.path.join(DIR_MAIN, 'data/weights/pruned')\n",
    "! mkdir {DIR_WEIGHTS}\n",
    "DEST_NAME_ZIP   = os.path.join(DIR_WEIGHTS, 'weights-prune-me.zip')\n",
    "DEST_NAME_FILES = DIR_WEIGHTS\n",
    "download_gdrive(FILE_ID, DEST_NAME_ZIP)\n",
    "! unzip {DEST_NAME_ZIP} -d {DEST_NAME_FILES}\n",
    "\n",
    "## Step5 - Generate .txt files for training/validation \n",
    "from src.dataloader import setup_VOC\n",
    "DIR_DATA = os.path.join(DIR_MAIN, 'data/dataset/')\n",
    "setup_VOC(DIR_DATA)\n",
    "DIR_DATA_VOC = os.path.join(DIR_DATA, 'VOCdevkit')\n",
    "! cat {DIR_DATA_VOC}/2007_train.txt {DIR_DATA_VOC}/2007_val.txt {DIR_DATA_VOC}/2012_*.txt > {DIR_DATA_VOC}/voc_train.txt\n",
    "\n",
    "## Step6 - Check which GPU\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "dir_main = os.path.abspath('CS4180-DL')\n",
    "sys.path.append(dir_main)\n",
    "print (' - In Path : ', sys.path[-1])\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print (' - USE_GPU : ', USE_GPU)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# from src.predict import *\n",
    "\n",
    "if (1):\n",
    "    DIR_PROJ         = 'CS4180-DL'\n",
    "\n",
    "if (1):\n",
    "    MODEL            = ''\n",
    "    MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "    MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/pruned/yolov2-voc-weight-prune-70.0.weights')\n",
    "    PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "    EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/2007_test.txt')\n",
    "    EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'eval_data')\n",
    "    EVAL_PREFIX      = 'iter1_weighspruned_70_'\n",
    "    EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'eval_results')\n",
    "    print (' - 0. MODEL : ', MODEL)\n",
    "    print (' - 0. EVAL_PREFIX : ', EVAL_PREFIX)\n",
    "\n",
    "\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL, MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "valObj.predict(BATCH_SIZE=32)\n",
    "# valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T09:29:08.502012Z",
     "start_time": "2019-05-28T09:28:59.393363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16551 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- init_epoch :  0\n",
      " -- max_epochs :  5\n",
      " ---------------------------- EPOCH :  0  ---------------------------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/16551 [00:00<2:20:53,  1.96it/s]/home/strider/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - data (or X) :  torch.Size([1, 3, 416, 416]) torch.float32\n",
      " - target (or Y) :  torch.Size([1, 250]) torch.float64\n",
      " - Total train points :  16551  || nsamples :  16551\n",
      " i : 0 / 16551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strider/anaconda3/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "  0%|          | 6/16551 [00:04<4:04:07,  1.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-71a63f51a6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrainObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOv2Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPASCAL_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPASCAL_VALID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_LOGDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_CFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_WEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, PASCAL_TRAIN, PASCAL_VALID, TRAIN_LOGDIR, MODEL_CFG, MODEL_WEIGHT)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                         \u001b[0;31m#target= target.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from src.train import YOLOv2Train\n",
    "from tensorboardcolab import TensorBoardColab\n",
    "\n",
    "if (1):\n",
    "    DIR_MAIN         = 'CS4180-DL'\n",
    "    print (' - 1. DIR_MAIN :  ', DIR_MAIN)\n",
    "    \n",
    "if (1):\n",
    "    PASCAL_DIR   = os.path.join(DIR_MAIN, 'data/dataset/VOCdevkit/')\n",
    "    PASCAL_TRAIN = os.path.join(DIR_MAIN, 'data/dataset/VOCdevkit/voc_train.txt')\n",
    "    PASCAL_VALID = os.path.join(DIR_MAIN, 'data/dataset/VOCdevkit/2007_test.txt')\n",
    "    TRAIN_LOGDIR = os.path.join(DIR_MAIN, 'train_data')\n",
    "    VAL_LOGDIR   = os.path.join(DIR_MAIN, 'eval_data')\n",
    "    VAL_OUTPUTDIR_PKL = os.path.join(DIR_MAIN, 'eval_results')\n",
    "    MODEL_CFG    = os.path.join(DIR_MAIN, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "    # MODEL_WEIGHT = os.path.join(DIR_MAIN, 'data/weights/pruned/yolov2-voc-weight-prune-30.0.weights')\n",
    "    MODEL_WEIGHT = os.path.join(DIR_MAIN, 'data/dataset/weights/yolov2-voc.weights')\n",
    "    print (' - 2. MODEL_WEIGHT :  ', MODEL_WEIGHT)\n",
    "    \n",
    "if (1):\n",
    "    VAL_PREFIX   = 'pretrained'\n",
    "    BATCH_SIZE   = 1;\n",
    "    print (' - 3. VAL_PREFIX : ', VAL_PREFIX)\n",
    "\n",
    "if (1):\n",
    "    try:\n",
    "        print (' - 4. Logger : ', LOGGER)\n",
    "    except:\n",
    "        LOGGER = TensorBoardColab()\n",
    "        print (' - 4. Logger : ', LOGGER)\n",
    "\n",
    "\n",
    "if (1):\n",
    "    trainObj = YOLOv2Train()\n",
    "    trainObj.train(PASCAL_DIR, PASCAL_TRAIN, PASCAL_VALID, TRAIN_LOGDIR, VAL_LOGDIR, VAL_OUTPUTDIR_PKL, VAL_PREFIX\n",
    "                   , MODEL_CFG, MODEL_WEIGHT\n",
    "                   , BATCH_SIZE\n",
    "                   , LOGGER)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
